# Local LLM Setup Guide

This guide explains how to configure Strix to use locally hosted large language models (LLMs) instead of cloud-based providers.

## Table of Contents

- [Overview](#overview)
- [Environment Variables](#environment-variables)
- [Ollama Setup](#ollama-setup)
- [LM Studio Setup](#lm-studio-setup)
- [OpenAI-Compatible APIs](#openai-compatible-apis)
- [Troubleshooting](#troubleshooting)

## Overview

Strix uses [LiteLLM](https://docs.litellm.ai/) as a unified interface to communicate with various LLM providers. This allows you to use local models served via Ollama, LM Studio, or any OpenAI-compatible API.

**Key benefits of local LLMs:**
- No cloud API costs
- Data stays on your network
- Works offline
- Full control over model selection

## Environment Variables

| Variable | Required | Description |
|----------|----------|-------------|
| `STRIX_LLM` | **Yes** | Model identifier in `provider/model` format |
| `LLM_API_BASE` | **Yes** (for local) | Base URL of your local LLM server |
| `LLM_API_KEY` | No | API key (optional for most local setups) |
| `LLM_TIMEOUT` | No | Request timeout in seconds (default: 600) |

**Alternative base URL variables** (checked in priority order):
1. `LLM_API_BASE`
2. `OPENAI_API_BASE`
3. `LITELLM_BASE_URL`
4. `OLLAMA_API_BASE`

## Ollama Setup

[Ollama](https://ollama.ai/) is the easiest way to run local LLMs.

### 1. Install and Start Ollama

```bash
# Install Ollama (Linux/macOS)
curl -fsSL https://ollama.ai/install.sh | sh

# Pull a model
ollama pull mistral

# Start the server (if not already running)
ollama serve
```

### 2. Configure Strix

Create a `.env` file or export these environment variables:

```bash
LLM_API_BASE="http://192.168.7.7:11434"
LLM_API_KEY="1234"
STRIX_LLM="ollama/mistral"
```

> **Note:** For Ollama, the API key can be any non-empty string (e.g., `1234`). Ollama doesn't require authentication but LiteLLM may expect a key to be set.

### 3. Verify Connection

```bash
# Test that Ollama is responding
curl http://192.168.7.7:11434/api/tags

# Run Strix
strix -t https://example.com/
```

### Common Ollama Models

| Model | STRIX_LLM Value | Notes |
|-------|-----------------|-------|
| Mistral 7B | `ollama/mistral` | Fast, good general purpose |
| Llama 3.1 8B | `ollama/llama3.1` | Meta's latest open model |
| CodeLlama | `ollama/codellama` | Optimized for code |
| Mixtral 8x7B | `ollama/mixtral` | High quality, requires more RAM |
| Qwen2.5 | `ollama/qwen2.5` | Strong reasoning capabilities |

### Network Configuration for Ollama

If running Ollama on a different machine, ensure it binds to all interfaces:

```bash
# Set environment variable before starting Ollama
OLLAMA_HOST=0.0.0.0 ollama serve
```

Or edit the Ollama service file:
```bash
# Linux systemd
sudo systemctl edit ollama

# Add:
[Service]
Environment="OLLAMA_HOST=0.0.0.0"
```

## LM Studio Setup

[LM Studio](https://lmstudio.ai/) provides a GUI for running local models.

### 1. Start the Local Server

1. Open LM Studio
2. Load your preferred model
3. Go to **Local Server** tab
4. Click **Start Server** (default port: 1234)

### 2. Configure Strix

```bash
LLM_API_BASE="http://localhost:1234/v1"
LLM_API_KEY="lm-studio"
STRIX_LLM="openai/local-model"
```

> **Note:** LM Studio uses the OpenAI-compatible format, so use `openai/` prefix for the model name.

## OpenAI-Compatible APIs

For any server that implements the OpenAI Chat Completions API:

```bash
LLM_API_BASE="http://your-server:port/v1"
LLM_API_KEY="your-api-key"
STRIX_LLM="openai/your-model-name"
```

### Examples

**vLLM Server:**
```bash
LLM_API_BASE="http://localhost:8000/v1"
LLM_API_KEY="token-abc123"
STRIX_LLM="openai/meta-llama/Llama-3.1-8B-Instruct"
```

**Text Generation Inference (TGI):**
```bash
LLM_API_BASE="http://localhost:8080/v1"
LLM_API_KEY="tgi"
STRIX_LLM="openai/tgi-model"
```

**LocalAI:**
```bash
LLM_API_BASE="http://localhost:8080/v1"
LLM_API_KEY="localai"
STRIX_LLM="openai/gpt-4"
```

## Troubleshooting

### Error: "Access Denied" or Proxy Errors

**Symptom:** Error message contains HTML about "Access Denied" or mentions a proxy/squid server:

```
Access control configuration prevents your request from being allowed at this time.
Generated by wuxshcsitd08801.novalocal (squid/4.9)
```

**Cause:** A corporate proxy or firewall is intercepting requests to your local LLM server.

**Solutions:**

1. **Bypass the proxy for local addresses:**
   ```bash
   export NO_PROXY="localhost,127.0.0.1,192.168.0.0/16,10.0.0.0/8"
   export no_proxy="localhost,127.0.0.1,192.168.0.0/16,10.0.0.0/8"
   ```

2. **Ensure the LLM server is accessible:**
   ```bash
   # Test direct connectivity (replace with your server IP/port)
   curl -v http://192.168.7.7:11434/api/tags
   ```

3. **Check firewall rules:** Ensure the LLM server port is open for incoming connections.

4. **Use localhost when possible:** If running Strix and the LLM on the same machine:
   ```bash
   LLM_API_BASE="http://127.0.0.1:11434"
   ```

### Error: "Connection Refused"

**Cause:** The LLM server is not running or not listening on the expected address/port.

**Solutions:**

1. Verify the server is running:
   ```bash
   # For Ollama
   curl http://localhost:11434/api/tags

   # For OpenAI-compatible servers
   curl http://localhost:8000/v1/models
   ```

2. Check the server is bound to the correct interface:
   - `127.0.0.1` = localhost only
   - `0.0.0.0` = all network interfaces

3. Verify the port number in your `LLM_API_BASE` matches the server configuration.

### Error: "Model Not Found"

**Cause:** The model specified in `STRIX_LLM` doesn't exist on the server.

**Solutions:**

1. For Ollama, list available models:
   ```bash
   ollama list
   ```

2. Pull the model if missing:
   ```bash
   ollama pull mistral
   ```

3. Ensure the model name format is correct: `provider/model`

### Error: "Timeout"

**Cause:** The model is taking too long to respond, especially on first load.

**Solutions:**

1. Increase the timeout:
   ```bash
   export LLM_TIMEOUT=1200  # 20 minutes
   ```

2. Warm up the model before running Strix:
   ```bash
   curl http://localhost:11434/api/generate -d '{"model": "mistral", "prompt": "Hello"}'
   ```

3. Use a smaller/faster model.

### Verifying Your Configuration

Before running Strix, verify your setup:

```bash
# 1. Check environment variables
echo "STRIX_LLM: $STRIX_LLM"
echo "LLM_API_BASE: $LLM_API_BASE"

# 2. Test LLM server connectivity
curl -s "${LLM_API_BASE}/models" | head -20

# 3. Test a simple completion (OpenAI-compatible)
curl -X POST "${LLM_API_BASE}/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer ${LLM_API_KEY}" \
  -d '{
    "model": "mistral",
    "messages": [{"role": "user", "content": "Say hello"}]
  }'
```

## Complete Configuration Examples

### Minimal Ollama Setup (Same Machine)

```bash
# .env file
STRIX_LLM="ollama/mistral"
LLM_API_BASE="http://localhost:11434"
LLM_API_KEY="ollama"
```

### Remote Ollama Server

```bash
# .env file
STRIX_LLM="ollama/llama3.1"
LLM_API_BASE="http://192.168.1.100:11434"
LLM_API_KEY="1234"
LLM_TIMEOUT=900
```

### Corporate Network with Proxy Bypass

```bash
# .env file
STRIX_LLM="ollama/mistral"
LLM_API_BASE="http://192.168.7.7:11434"
LLM_API_KEY="1234"

# Shell environment (add to ~/.bashrc)
export NO_PROXY="192.168.7.7,localhost,127.0.0.1"
export no_proxy="192.168.7.7,localhost,127.0.0.1"
```

## See Also

- [LiteLLM Provider Documentation](https://docs.litellm.ai/docs/providers)
- [Ollama Documentation](https://github.com/ollama/ollama)
- [Strix Configuration](../README.md#configuration)
